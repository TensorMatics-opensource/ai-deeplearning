{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasten your seatbelt\n",
    "\n",
    "We are assuming that you have already fasten your seat belt(setup the docker) for the text classification development ride. if you haven't please visit the link https://docs.google.com/document/d/13rcF4ewux1ZxErKLHsZZALODRBqxuS7Bz0PmveMIhK0/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the steps in a typical ML cycle\n",
    "1. Collect Data\n",
    "2. Preprocess \n",
    "3. Build Model\n",
    "4. Deploy it\n",
    "5. Consume it\n",
    "6. Update model to improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Either Bring your own data \n",
    "\n",
    "b) OR use from our scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring your own data\n",
    "\n",
    "This is an data ingestion functionality to get you started faster and \n",
    "\n",
    "a. keep it at required relative path on your host OS // this will perform create directory and move operation\n",
    "\n",
    "b. **(Recommended)** Keep data in your third party cloud in GCS/S3/Azure  // this will perform upload operation  upload_data\n",
    "\n",
    "c. You already have an FTP/Googledrive/DropBox and you need to bring this data to local or take this to third party cloud  //Here download_data('ftp_details','gcs')\n",
    "\n",
    "So different patterns of this function would work like this\n",
    "import tensormatics as tm\n",
    "```\n",
    "tm.download_data('source_config.json','destination_config.json')\n",
    "tm.download_data('source_config_file.json','destination_config_file.json')\n",
    "```\n",
    "\n",
    "sources --> destination pairs allowed\n",
    "1. local --> S3/GCS/Azure/local_required_folder\n",
    "2. FTP/DropBox/GoogleDrive  --> S3/GCS/Azure\n",
    "3. S3/GCS/Azure --> S3/GCS/Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download it from our scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list datasets already available by scraping community\n",
    "\n",
    "This scraper of tensormatics already provides you multiple datasets which are available based on the keywords that you provide\n",
    "\n",
    "```\n",
    "list_datasets({\"keywords\": [\"travel\", \"delhi\", \"\"] , \"framework\": \"tensorflow\"})\n",
    "list_datasets({\"keywords\": [\"travel\", \"delhi\", \"\"] , \"framework\": \"tensormatics\"})\n",
    "list_datasets({\"keywords\": [\"travel\", \"delhi\", \"\"] , \"framework\": \"fastai\"})\n",
    "```\n",
    "\n",
    "//todo Few capabilities that it should have is to provide details into the dataset with some functionality like some function as below\n",
    "```\n",
    "view_dataset(\"dataset_name\",\"framework\":\"tensorflow\")\n",
    "view_meta(\"dataset_name\",\"framework\":\"tensorflow\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape if you are not able to find data\n",
    "\n",
    "if you didn't find any dataset, now its the time to get the text data from scraper\n",
    "\n",
    "Scraper call can be \n",
    "```\n",
    "tm.list_scrapers({\"data_type\":\"text\", \"keywords\":[\"travel\",\"delhi\"])  // optional arguments need to be saved as 1 file per text record or 1 file with all text records\n",
    "tm.scrape_data('scraper','local') //this means it will download to local\n",
    "tm.scrape_data('Newspaper3k','S3', share_with='community')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is now available in local_required_folder/S3/GCS/Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario when data is on local_required_folder and models need to be built with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario when data is on S3 and models need to be built with sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario when data is on blobstore and models need to be built with Azure AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scenario when data is on GCS and models need to be built with Google AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "\n",
    "```\n",
    "tm.dataset.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling the data\n",
    "\n",
    "label the data if not labelled. push the data into labeller and label it and data needs to be exported into google automl required format\n",
    "\n",
    "```\n",
    "tm.push_for_labelling('labellerr',{'username':'puneet.jindal@tensormatics.com','password':'xyz'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the data before cosumption into models\n",
    "create CSV in the required format for the google automl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training and deployment\n",
    "### scenario when data is on GCS in the automl required format\n",
    "you now have 4 options\n",
    "1. run pre-trained models from GCP ai platform - API hit - just for exploration but can't customize\n",
    "2. run google automl from GCP ai platform - SAAS but can customize for beginners\n",
    "3. train with ai platform ml engine with your own code from gcp ai platform - PAAS for medium\n",
    "4. train on compute engine - IAAS for experts, this is very similar to developing on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosume models and plot the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
